{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c325c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce84e0092f6493484c60a7904515081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Integrated Gemini Chatbot with RAG (type 'exit' to quit)\n",
      "Bot (RAG): Hi there! How can I help you today?\n",
      "\n",
      "Bot (RAG): The provided text focuses on shipping and delivery options.  There is no information about singing capabilities.  Therefore, the answer is **no**.\n",
      "\n",
      "Bot (RAG): We offer standard, expedited, and express shipping through FedEx, UPS, and USPS.  Free standard shipping is available on orders over $50 within the continental U.S.  However, some items (batteries, aerosols, perishables) may have shipping restrictions.\n",
      "\n",
      "Bot (RAG): No, you cannot ship children.  Shipping children is illegal and incredibly dangerous.\n",
      "\n",
      "Bot (RAG): Based on the provided text, you can ship most items, but there are restrictions on batteries, aerosols, and perishables.  The exact restrictions aren't detailed.  To know for sure if a specific item is shippable, you would need to check with the company directly or review their shipping policy.\n",
      "\n",
      "Bot (RAG): You're welcome!  Is there anything else I can help you with?\n",
      "\n",
      "Bot (RAG): The provided context gives answers to questions about shipping delays, combining orders, and rerouting packages.  There is no question in the context that has \"no\" as an answer.  Therefore, there is no answer to provide.\n",
      "\n",
      "Bot: Goodbye!\n",
      "\n",
      "Summary:\n",
      " The user initially asked the bot to sing, then inquired about shipping options.  The bot explained its shipping options (standard, expedited, express), noting restrictions on certain items.  Crucially, the bot clarified that shipping children is illegal.  The conversation ended with the user expressing satisfaction.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCSypgJaG3XLvlJvbDg_kg5RbzZm4vf9B8\"   # API KEY(CREATE .env file and put it there)\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "with open('faq.json', 'r') as f:\n",
    "    data = json.load(f)                                                     #reading faq file to implement rag, we can add more sources need to discuss \n",
    "\n",
    "documents = [f\"Q: {item['question']}\\nA: {item['answer']}\" for item in data]\n",
    "metadatas = [{\"question\": item[\"question\"]} for item in data]\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")                          #adding vector embeddings to the faqs and storing them in chromadb\n",
    "embeddings = embedder.encode(documents, show_progress_bar=True)             #used miniLM cuz its free and lightweight and good semantic understanding\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_faq\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"faq\")\n",
    "\n",
    "if len(collection.get()['ids']) == 0:\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        embeddings=embeddings.tolist(),\n",
    "        metadatas=metadatas,\n",
    "        ids=[str(i) for i in range(len(documents))]\n",
    "    )\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")                   #gemini model for our chat bot\n",
    "chat = model.start_chat(history=[])                                 #we store the chat history\n",
    "\n",
    "def generate_gemini_answer(query, k=3, similarity_threshold=0.6):           # top 3 responses with 60%or more similarity\n",
    "    query_embedding = embedder.encode([query])[0]                           \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=k\n",
    "    )\n",
    "    \n",
    "    if not results['documents'][0]:\n",
    "        return \"I'm sorry, I couldn't find any relevant information to answer your question.\"\n",
    "    if len(results['documents'][0]) == 0:\n",
    "        return \"I'm sorry, I couldn't find any relevant information to answer your question.\"\n",
    "    \n",
    "    context = \"\\n\\n\".join(results['documents'][0])\n",
    "\n",
    "    prompt = f\"\"\"Answer the following question based on the context provided.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "#CHATBOT STARTS HERE\n",
    "\n",
    "chat_history_log = []\n",
    "\n",
    "print(\"ðŸ¤– Integrated Gemini Chatbot with RAG (type 'exit' to quit)\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:                  #TO EXIT CONVO\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    chat_history_log.append(f\"User: {user_input}\")              #Storing input give by the user\n",
    "\n",
    "    rag_response = generate_gemini_answer(user_input)           #generating ans with context integration\n",
    "    if rag_response:\n",
    "        print(\"Bot (RAG):\", rag_response)\n",
    "        chat_history_log.append(f\"Bot (RAG): {rag_response}\")       #stroing responses \n",
    "    else:\n",
    "        response = chat.send_message(user_input)\n",
    "        print(\"Bot:\", response.text)\n",
    "        chat_history_log.append(f\"Bot: {response.text}\")\n",
    "\n",
    "with open(\"chat_history.txt\", \"w\", encoding=\"utf-8\") as f:              #storing in a txt file all the chats\n",
    "    for line in chat_history_log:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "#SUMMARIZING ONCE THE CONVO IS OVER ANS STORING IT WHICH CAN BE RETRIVED WHEN TICKET IS BEING CREATED\n",
    "\n",
    "with open(\"chat_history.txt\", \"r\", encoding=\"utf-8\") as f:  \n",
    "    chat_text = f.read()\n",
    "\n",
    "prompt = f\"Summarize the following conversation briefly:\\n\\n{chat_text}\"\n",
    "summary_response = model.generate_content(prompt)\n",
    "print(\"\\nSummary:\\n\", summary_response.text)\n",
    "with open(\"summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary_response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exoplanet_prj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
